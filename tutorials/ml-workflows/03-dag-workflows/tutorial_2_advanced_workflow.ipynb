{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5064a94",
   "metadata": {},
   "source": [
    "# DAG workflows\n",
    "\n",
    "**Author(s)**: Matteo Bunino (CERN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588e0f6a",
   "metadata": {},
   "source": [
    "In the first two tutorials we saw how to define simple sequential workflows by\n",
    "means of the Pipeline object, which feds the outputs of the previous component\n",
    "as inputs of the following one.\n",
    "In this tutorial we show how to create more complex workflows, with\n",
    "non-sequential data flows. Here, components can be arranges as an directed\n",
    "acyclic graph (DAG). Under the DAG assumption, outputs of each block can be fed\n",
    "as input potentially to any other component, granting great flexibility to the\n",
    "experimenter.\n",
    "The trade-off for improved flexibility is a change in the way we define\n",
    "configuration files. From now on, it will only be possible to configure the\n",
    "parameters used by the training script, but not its structure through the\n",
    "Pipeline.\n",
    "\n",
    "Here you can find a graphical representation of the DAG workflow implemented below:\n",
    "![dag_wf](Advanced_workflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c30284ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from itwinai.components import Predictor, monitor_exec\n",
    "\n",
    "from basic_components import (\n",
    "    MyDataGetter, MyDatasetSplitter, MyTrainer, MySaver\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "826fe8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnsemblePredictor(Predictor):\n",
    "    @monitor_exec\n",
    "    def execute(self, dataset, model_ensemble) -> Any:\n",
    "        \"\"\"\n",
    "        do some predictions with model on dataset...\n",
    "        \"\"\"\n",
    "        return dataset\n",
    "\n",
    "# Parameters\n",
    "DATA_SIZE = 123\n",
    "TRAIN_PROP = .5\n",
    "VALIDATION_PROP = 0.2\n",
    "LR_MODEL1 = 1e-3\n",
    "LR_MODEL2 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66209fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################\n",
      "# Starting execution of 'MyDataGetter'... #\n",
      "###########################################\n",
      "#####################################\n",
      "# 'MyDataGetter' executed in 0.000s #\n",
      "#####################################\n",
      "################################################\n",
      "# Starting execution of 'MyDatasetSplitter'... #\n",
      "################################################\n",
      "##########################################\n",
      "# 'MyDatasetSplitter' executed in 0.000s #\n",
      "##########################################\n",
      "########################################\n",
      "# Starting execution of 'MyTrainer'... #\n",
      "########################################\n",
      "##################################\n",
      "# 'MyTrainer' executed in 0.000s #\n",
      "##################################\n",
      "########################################\n",
      "# Starting execution of 'MyTrainer'... #\n",
      "########################################\n",
      "##################################\n",
      "# 'MyTrainer' executed in 0.000s #\n",
      "##################################\n",
      "######################################\n",
      "# Starting execution of 'MySaver'... #\n",
      "######################################\n",
      "################################\n",
      "# 'MySaver' executed in 0.000s #\n",
      "################################\n",
      "##################################################\n",
      "# Starting execution of 'MyEnsemblePredictor'... #\n",
      "##################################################\n",
      "############################################\n",
      "# 'MyEnsemblePredictor' executed in 0.000s #\n",
      "############################################\n",
      "\n",
      "Predictions: [85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122]\n"
     ]
    }
   ],
   "source": [
    "# Define workflow components\n",
    "getter = MyDataGetter(data_size=DATA_SIZE)\n",
    "splitter = MyDatasetSplitter(\n",
    "    train_proportion=TRAIN_PROP,\n",
    "    validation_proportion=VALIDATION_PROP,\n",
    "    test_proportion=1 - TRAIN_PROP - VALIDATION_PROP\n",
    ")\n",
    "trainer1 = MyTrainer(lr=LR_MODEL2)\n",
    "trainer2 = MyTrainer(lr=LR_MODEL1)\n",
    "saver = MySaver()\n",
    "predictor = MyEnsemblePredictor(model=None)\n",
    "\n",
    "# Define ML workflow\n",
    "dataset = getter.execute()\n",
    "train_spl, val_spl, test_spl = splitter.execute(dataset)\n",
    "_, _, _, trained_model1 = trainer1.execute(train_spl, val_spl, test_spl)\n",
    "_, _, _, trained_model2 = trainer2.execute(train_spl, val_spl, test_spl)\n",
    "_ = saver.execute(trained_model1)\n",
    "predictions = predictor.execute(test_spl, [trained_model1, trained_model2])\n",
    "print()\n",
    "print(\"Predictions: \" + str(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
