# Below we detail the configuration fields for the SLURM scheduler

# Name for the SLURM job
job_name: my_slurm_job
# SLURM account that will be charged
account: intertwin
# SLURM partition
partition: develbooster
# Maximum job lifetime. Format HH:MM:SS
time: "00:11:11"

# Output and error log files:
# Keep in mind that these will be overwritten if "mode" is not "single", and that
# if you override the dist_strat in the CLI, then these will already have evaluated
# and thus might not correspond. Thus, we suggest you only change the dist_strat in
# the config and avoid overriding it in the CLI.
std_out: slurm_job_logs/${dist_strat}-${num_nodes}x${gpus_per_node}.out
err_out: slurm_job_logs/${dist_strat}-${num_nodes}x${gpus_per_node}.err

# Number of nodes to allocate
num_nodes: 1
# Number of GPUs allocated per node
gpus_per_node: 4
# Number of CPUs allocated per SLURM task
cpus_per_task: 16
# Memory allocation
memory: 16G
# Exclusive allocation (i.e., #SBATCH --exclusive)
exclusive: false


# --------------------------------------------------------------------------------
# Below this line we detail the configuration for the exec-pipeline command

# The distributed strategy can be "ddp", "deepspeed" or "horovod"
dist_strat: ddp
# Path to python virtual environment (if needed)
python_venv: .venv
# Experiment name
exp_name: my_experiment
# Name of a run belonging to some experiment
run_name: my_run
# Key of the itwinai pipeline to select in the config file 
pipe_key: training_pipeline
# Name of the config file
config_name: config
# Path to the directory containing the configuration file
config_path: .

# Make sure the below strategy matches the one above
training_cmd: | 
  $(which itwinai) exec-pipeline \
  --config-name={config_name} \
  --config-path={config_path} \
  --pipe-key={pipe_key} \
  strategy={dist_strat} \
  experiment_name={experiment_name} \
  run_name={run_name}
