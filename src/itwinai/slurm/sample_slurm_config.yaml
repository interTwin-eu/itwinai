job_name: my_slurm_job

account: intertwin
partition: develbooster

# HH:MM:SS
time: 00:11:11

# Keep in mind that these will be overwritten if "mode" is not "single", and that
# if you override the dist_strat in the CLI, then these will already have evaluated
# and thus might not correspond. Thus, we suggest you only change the dist_strat in
# the config and avoid overriding it in the CLI.
std_out: slurm_job_logs/${dist_strat}-${num_nodes}x${gpus_per_node}.out
err_out: slurm_job_logs/${dist_strat}-${num_nodes}x${gpus_per_node}.err

num_nodes: 1
num_tasks_per_node: 1
gpus_per_node: 4
cpus_per_task: 16
memory: 16G

# The distributed strategy can be "ddp", "deepspeed" or "horovod"
dist_strat: ddp
python_venv: .venv
experiment_name: my_experiment
run_name: my_run

# Make sure the below strategy matches the one above
training_cmd: | 
  $(which itwinai) exec-pipeline \
  --config config.yaml \
  --pipe-key rnn_training_pipeline \
  strategy={dist_strat} \
  experiment_name={experiment_name} \
  run_name={run_name}
