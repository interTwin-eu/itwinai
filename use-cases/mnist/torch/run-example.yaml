# Default fields (always needed)
strategy: ddp
run_name: "mnist"

# General config
dataset_root: .tmp/
num_classes: 10
batch_size: 128
num_workers_dataloader: 4
pin_memory: False
lr: 0.001
momentum: 0.9
fp16_allreduce: False
use_adasum: False
gradient_predivide_factor: 1.0
epochs: 5
test_data_path: mnist-sample-data
inference_model_mlflow_uri: mnist-pre-trained.pth
predictions_dir: mnist-predictions
predictions_file: predictions.csv
class_labels: null
checkpoints_location: checkpoints
checkpoint_every: 1

plugins:
  - git+https://github.com/matbun/itwinai-mnist-plugin.git

slurm_config:
  job_name: mnist-job
  account: s24r05-03-users
  partition: gpu
  memory: 64G
  mode: single
  num_nodes: 2
  submit_job: true
  save_script: true
  pre_exec_file: https://raw.githubusercontent.com/interTwin-eu/itwinai/refs/heads/main/src/itwinai/slurm/system-base-scripts/vega_pre_exec.sh

  # Fields referring to this config file
  pipe_key: training_pipeline
  config_name: run-example # Assuming this is the name of this file
  config_path: . # Assuming that run-example.yaml is in the current directory

  # Propagate global config to this slurm_config
  distributed_strategy: ${strategy}
  run_name: ${run_name}

  # This provides a template for the training command launched
  # using itwinai exec-pipeline -c <this config>.yaml
  # The template is filled in using the fields of this slurm_config.
  training_cmd: |
    "{itwinai_launcher} exec-pipeline "
    "--config-name={config_name} "
    "--config-path={config_path} "
    "--strategy={distributed_strategy} "
    "--run_name={run_name}"
    "+pipe_key={pipe_key} "

# Workflows configuration
training_pipeline:
  _target_: itwinai.pipeline.Pipeline
  steps:
    dataloading_step:
      _target_: itwinai.plugins.mnist.dataloader.MNISTDataModuleTorch
      save_path: ${dataset_root}
    training_step:
      _target_: itwinai.torch.trainer.TorchTrainer
      measure_gpu_data: False
      enable_torch_profiling: False
      store_torch_profiling_traces: False
      measure_epoch_time: False
      time_ray: True # track time for ray report and fit
      # from_checkpoint: ${itwinai.cwd:}/checkpoints_ddp/best_model/
      config:
        batch_size: ${batch_size}
        num_workers_dataloader: ${num_workers_dataloader}
        pin_gpu_memory: ${pin_memory}
        optimizer: sgd
        optim_lr: ${lr}
        optim_momentum: ${momentum}
        fp16_allreduce: ${fp16_allreduce}
        use_adasum: ${use_adasum}
        gradient_predivide_factor: ${gradient_predivide_factor}
      ray_scaling_config:
        _target_: ray.train.ScalingConfig
        num_workers: 2
        use_gpu: true
        resources_per_worker:
          CPU: 11
          GPU: 1 # can be 0.0 < 1.0 as long as num_workers set to 1
      ray_tune_config:
        _target_: ray.tune.TuneConfig
        num_samples: 4
        reuse_actors: True
        scheduler:
          _target_: ray.tune.schedulers.ASHAScheduler
          metric: loss
          mode: min
          grace_period: 5
          reduction_factor: 6
          brackets: 1
      ray_run_config:
        _target_: ray.tune.RunConfig
        # storage_path must be an absolute path. Defaulting to the directory from which the
        # job is launched using the itwinai custom OmegaConf resolver ${itwinai.cwd:}
        storage_path: ${itwinai.cwd:}/ray_checkpoints
        name: MNIST-HPO-Experiment
      ray_search_space:
        batch_size:
          type: choice
          categories: [64, 128]
        optim_lr:
          type: uniform
          lower: 1e-5
          upper: 1e-3
      model:
        _target_: itwinai.plugins.mnist.model.Net
      epochs: ${epochs}
      metrics:
        accuracy:
          _target_: torchmetrics.classification.MulticlassAccuracy
          num_classes: ${num_classes}
        precision:
          _target_: torchmetrics.classification.MulticlassPrecision
          num_classes: ${num_classes}
        recall:
          _target_: torchmetrics.classification.MulticlassRecall
          num_classes: ${num_classes}
      logger:
        _target_: itwinai.loggers.LoggersCollection
        loggers:
          - _target_: itwinai.loggers.ConsoleLogger
            log_freq: 10000
          - _target_: itwinai.loggers.MLFlowLogger
            experiment_name: MNIST classifier
            log_freq: epoch
            tracking_uri: null
      strategy: ${strategy}
      checkpoint_every: ${checkpoint_every}
      checkpoints_location: ${checkpoints_location}
