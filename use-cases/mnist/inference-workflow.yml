# Load with OmegaConf

# Other configuration files to merge with this file via OmegaConf 
conf-dependencies:
  - meta.yml

steps:
  - preprocessing:
      doc: Download and split MNIST dataset into train and test sets
      command: python ${root}/mnist-preproc.py
      env: 
        file: ${root}/env-files/preproc-env.yml
        prefix: ${root}/.venv-preproc
      args:
        output: ${datasets.preproc-images.location}
        stage: test
  - run-mlflow-server:
      doc: Run MLFlow server on localhost
      command: python ${root}/mlflow-server.py
      env: 
        file: ${ai-root}/env-files/pytorch-lock.yml
        prefix: ${ai-root}/.venv-pytorch
        source: ${ai-root}
      args:
        path: ${datasets.mlflow-backend-store-uri.location}
        port: ${mlflow.port}
  - ml-inference:
      doc: Apply a pre-trained neural network on unseen data and store them
      command: itwinai predict
      env: 
        file: ${ai-root}/env-files/pytorch-lock.yml
        prefix: ${ai-root}/.venv-pytorch
        source: ${ai-root}
      args:
        config: ${root}/mnist-ai-inference.yml
        input-dataset: ${datasets.preproc-images.location}
        predictions-location: ${datasets.ml-predictions.location}
        ml-logs: ${datasets.ml-logs.location}
  - stop-mlflow-server:
      doc: Stop MLFlow server on localhost, if running
      command: python ${root}/mlflow-server.py
      env: 
        file: ${ai-root}/env-files/pytorch-lock.yml
        prefix: ${ai-root}/.venv-pytorch
        source: ${ai-root}
      args:
        mode: kill
        port: ${mlflow.port}

