# Configuration file of AI workflows for MNIST use case
# Load with OmegaConf

# Pytorch lightning config for training
train:
  type: lightning
  # Follows lightning config file format:
  # https://pytorch-lightning.readthedocs.io/en/1.6.5/common/lightning_cli.html#multiple-models-and-or-datasets
  conf:
    seed_everything: 4231162351

    # Lightning Trainer configuration
    trainer:
      # Set to "cpu" when using pytorch "cpuonly" package
      accelerator: auto
      strategy: auto
      devices: auto
      num_nodes: 1
      precision: 32-true
      
      # MLFlow logger (initial) configuration.
      # Do not modify this field
      logger:
        class_path: lightning.pytorch.loggers.MLFlowLogger
        init_args:
          experiment_name: ${logger.experiment_name}
          run_name: null
          tracking_uri: null
          tags: null
          save_dir: ./mlruns
          log_model: false
          prefix: ''
          artifact_location: null
          run_id: null
      
      # Callbacks
      callbacks:
        - class_path: lightning.pytorch.callbacks.early_stopping.EarlyStopping
          init_args:
            monitor: val_loss
            patience: 2
        - class_path: lightning.pytorch.callbacks.lr_monitor.LearningRateMonitor
          init_args:
            logging_interval: step
        - class_path: lightning.pytorch.callbacks.ModelCheckpoint
          init_args:
            dirpath: checkpoints
            filename: best-checkpoint
            save_top_k: 1
            verbose: true
            monitor: val_loss
            mode: min

      fast_dev_run: false
      max_epochs: 1
      min_epochs: null
      max_steps: -1
      min_steps: null
      max_time: null
      limit_train_batches: null
      limit_val_batches: null
      limit_test_batches: null
      limit_predict_batches: null
      overfit_batches: 0.0
      val_check_interval: null
      check_val_every_n_epoch: 1
      num_sanity_val_steps: null
      log_every_n_steps: null
      enable_checkpointing: null
      enable_progress_bar: null
      enable_model_summary: null
      accumulate_grad_batches: 1
      gradient_clip_val: null
      gradient_clip_algorithm: null
      deterministic: null
      benchmark: null
      inference_mode: true
      use_distributed_sampler: true
      profiler: null
      detect_anomaly: false
      barebones: false
      plugins: null
      sync_batchnorm: false
      reload_dataloaders_every_n_epochs: 0
      default_root_dir: null

    # Lightning Model configuration
    model:
      class_path: itwinai.plmodels.mnist.LitMNIST
      init_args:
        hidden_size: 64

    # Lightning data module configuration
    data:
      class_path: itwinai.plmodels.mnist.MNISTDataModule
      init_args:
        data_dir: ${cli.train_dataset}
        batch_size: 32

    # Torch Optimizer configuration
    optimizer:
      class_path: torch.optim.AdamW
      init_args:
        lr: 0.001

    # Torch LR scheduler configuration
    lr_scheduler:
      class_path: torch.optim.lr_scheduler.ExponentialLR
      init_args:
        gamma: 0.1

# Mlflow
logger:
  experiment_name: MNIST classification lite
  description: A MLP classifier for MNIST dataset.
  log_every_n_epoch: 1
  log_every_n_steps: 1
  # Name used in Models Registry. If given, it is automatically
  # registered in the Models Registry.
  registered_model_name: MNIST-clf-lite
