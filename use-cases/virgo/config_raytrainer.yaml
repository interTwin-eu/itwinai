root_folder: /p/scratch/intertwin/datasets/virgo
data_root: ./data
epochs: 2
batch_size: 3
learning_rate: 0.0001
strategy: ddp
checkpoint_path: checkpoints/epoch_{}.pth

# To use the entire synthetic dataset that is generated prior to running the pipeline
training_pipeline:
  class_path: itwinai.pipeline.Pipeline
  init_args:
    steps:
      - class_path: data.TimeSeriesDatasetSplitter
        init_args:
          train_proportion: 0.9
          validation_proportion: 0.1
          rnd_seed: 42
          root_folder: ${root_folder}
      - class_path: trainer_ray.NoiseGeneratorTrainer
        init_args:
          config:
            scaling_config:
                num_workers: 2
                use_gpu: true
                resources_per_worker:
                  CPU: 5
                  GPU: 1
            tune_config:
              num_samples: 2
              scheduler:
                name: asha
                max_t: 10
                grace_period: 5
                reduction_factor: 4
                brackets: 1
            run_config:
              storage_path: ray_checkpoints
              name: Virgo-HPO-Experiment
            train_loop_config:
              batch_size: 
                type: choice
                options: [32, 64, 128]
              learning_rate:
                type: uniform
                min: 1e-5
                max: 1e-3
              epochs: 2
              generator: simple #unet
              loss: L1
              save_best: false
              shuffle_train: true
              random_seed: 17
              tracking_uri: mllogs/mlflow
              experiment_name: Virgo-HPO-Experiment
          strategy: ${strategy}
          logger:
            class_path: itwinai.loggers.LoggersCollection
            init_args:
              loggers:
                - class_path: itwinai.loggers.MLFlowLogger
                  init_args:
                    experiment_name: MNIST Ray HPO Experiment
                    log_freq: batch 

                
          
          
# To use a small dataset that is generated within the pipeline
training_pipeline_small:
  class_path: itwinai.pipeline.Pipeline
  init_args:
    steps:
      - class_path: data.TimeSeriesDatasetGenerator
        init_args:
          data_root: ${data_root}
      - class_path: data.TimeSeriesDatasetSplitterSmall
        init_args:
          train_proportion: 0.9
          validation_proportion: 0.1
          rnd_seed: 42
      - class_path: data.TimeSeriesProcessorSmall          
      - class_path: trainer_ray.NoiseGeneratorTrainer
        init_args:
          config:
            scaling_config:
                num_workers: 2
                use_gpu: true
                resources_per_worker:
                  CPU: 5
                  GPU: 1
            tune_config:
              num_samples: 8
              scheduler:
                name: asha
                max_t: 10
                grace_period: 5
                reduction_factor: 4
                brackets: 1
            run_config:
              storage_path: ray_checkpoints
              name: Virgo-HPO-Experiment
            train_loop_config:
              batch_size: 
                type: choice
                options: [32, 64, 128]
              learning_rate:
                type: uniform
                min: 1e-5
                max: 1e-3
              epochs: 30
              generator: simple #unet
              loss: L1
              save_best: false
              shuffle_train: true
              random_seed: 17
              tracking_uri: mllogs/mlflow
              experiment_name: Virgo-HPO-Experiment
          strategy: ${strategy}
          logger:
            class_path: itwinai.loggers.LoggersCollection
            init_args:
              loggers:
                - class_path: itwinai.loggers.MLFlowLogger
                  init_args:
                    experiment_name: Virgo Ray HPO Experiment
                    log_freq: batch 
