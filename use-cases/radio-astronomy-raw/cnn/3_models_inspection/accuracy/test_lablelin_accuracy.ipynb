{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Utility Functions\n",
    "\n",
    "This cell defines several utility functions used later in the notebook for calculating metrics (get_metrics_for_class), plotting confusion matrix (plot_confusion_matrix), plotting metrics (plot_metrics), and plotting metrics for all classes (plot_metrics_for_all_classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_MODEL_LABELS = '../models_labels/'\n",
    "PATH4IMAGES = '../../2_training/data/images/'\n",
    "\n",
    "def get_metrics_for_class(y_true, y_pred, target_class):\n",
    "    \"\"\"\n",
    "    Computes the True Positive, False Positive, False Negative, and True Negative\n",
    "    for a given class in a classification task.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (array-like): The true labels of the data.\n",
    "        y_pred (array-like): The predicted labels of the data.\n",
    "        target_class (int): The class for which the metrics are computed.\n",
    "\n",
    "    Returns:\n",
    "        tp (int): Number of True Positives.\n",
    "        fp (int): Number of False Positives.\n",
    "        fn (int): Number of False Negatives.\n",
    "        tn (int): Number of True Negatives.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the target_class is out of range of classes in the confusion matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if target_class >= len(cm):\n",
    "        raise ValueError(\"Target class is out of range.\")\n",
    "    \n",
    "    tp = cm[target_class, target_class]\n",
    "    fp = cm[:, target_class].sum() - tp\n",
    "    fn = cm[target_class, :].sum() - tp\n",
    "    tn = cm.sum() - fp - fn - tp\n",
    "\n",
    "    return tp, fp, fn, tn\n",
    "\n",
    "def plot_confusion_matrix(data, ax, title):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix on a given axis object with specific title.\n",
    "    \n",
    "    Parameters:\n",
    "        data (array-like): 2D array representing the confusion matrix.\n",
    "        ax (matplotlib.axes._axes.Axes): Matplotlib axis object to plot the confusion matrix on.\n",
    "        title (str): Title for the confusion matrix plot.\n",
    "\n",
    "    Notes:\n",
    "        - The axis ticks are turned off in this function.\n",
    "        - Text colors in each cell are determined by the cell's value relative to the sum of TP and TN values.\n",
    "    \"\"\"\n",
    "\n",
    "    labels = ['TP', 'FP', 'FN', 'TN']\n",
    "    im = ax.imshow(data, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, f\"{labels[2*i + j]}: {data[i][j]}\",\n",
    "                     ha=\"center\", va=\"center\", fontsize=16,\n",
    "                     color=\"white\" if data[i][j] > (data[0][0] + data[1][1]) / 2. else \"black\")\n",
    "\n",
    "            \n",
    "def plot_metrics(models, model_names, metrics, label):\n",
    "    \"\"\"\n",
    "    Plots multiple bars representing scores of various models for different metrics.\n",
    "    \n",
    "    Parameters:\n",
    "        models (list): List of lists representing scores for each model.\n",
    "        model_names (list): List of strings representing the names of the models.\n",
    "        metrics (list): List of strings representing the names of the metrics.\n",
    "        label (str): Label used for saving the plot.\n",
    "        \n",
    "    Notes:\n",
    "        - Each bar in the plot is color-coded and labeled according to the model it represents.\n",
    "        - The plot is saved as an image file with specified dpi.\n",
    "    \"\"\"\n",
    "\n",
    "    barWidth = 0.15\n",
    "    r = list(range(len(metrics)))\n",
    "    colors = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black', 'purple', 'grey']\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for i, model in enumerate(models):\n",
    "        plt.bar([x + barWidth*i for x in r], model, width=barWidth, label=model_names[i], color=colors[i])\n",
    "\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.grid(True, ls='--', color='gray', alpha=0.4)\n",
    "    plt.xticks([r + barWidth for r in range(len(models[0]))], metrics)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{PATH4IMAGES}{label}.jpg', format='jpg', dpi=350)\n",
    "    \n",
    "\n",
    "def plot_metrics_for_all_classes(y_true, y_pred, model_name, labels):\n",
    "    \"\"\"\n",
    "    Plots precision, recall, and F1 score for all classes.\n",
    "    \n",
    "    Parameters:\n",
    "        y_true (array-like): The true labels of the data.\n",
    "        y_pred (array-like): The predicted labels of the data.\n",
    "        model_name (str): The name of the model used for prediction.\n",
    "        labels (list): List of unique labels in the data.\n",
    "\n",
    "    Notes:\n",
    "        - The function calculates metrics specifically for each class and plots them in separate subplots.\n",
    "        - The title of each subplot includes the name and the count of the corresponding class in true labels.\n",
    "        - The plot is saved as an image file with specified dpi.\n",
    "    \"\"\"\n",
    "    metrics_names = ['Precision', 'Recall', 'F1 Score']\n",
    "        \n",
    "    replacement_dict_inverted = {v: k for k, v in replacement_dict.items()} \n",
    "    n_rows = 2\n",
    "    n_cols = 4\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    value_counts = y_true.value_counts()\n",
    "\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(16, 8))\n",
    "    fig.suptitle(f'{model_name}, accuracy: {accuracy:.2f}', fontsize=16)\n",
    "    \n",
    "    for idx, class_label in enumerate(labels):\n",
    "        row = idx // n_cols\n",
    "        col = idx % n_cols\n",
    "\n",
    "        # Calculate metrics for the specific class\n",
    "        \n",
    "        precision = precision_score(y_true, y_pred, labels=[class_label], average='macro', zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, labels=[class_label], average='macro', zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, labels=[class_label], average='macro', zero_division=0)\n",
    "\n",
    "        metrics_scores = [precision, recall, f1]\n",
    "\n",
    "        axs[row, col].bar(metrics_names, metrics_scores, color='b', alpha=0.7, label=f'Class {class_label}')\n",
    "        axs[row, col].set_ylim([0, 1.1])\n",
    "        for i, v in enumerate(metrics_scores):\n",
    "            axs[row, col].text(i, v + 0.02, f\"{v:.2f}\", color='black', ha='center')\n",
    "        axs[row, col].set_title(f'Metrics for {replacement_dict_inverted[class_label]} ({value_counts.get(class_label)})')\n",
    "        axs[row, col].set_ylabel('Score')\n",
    "        axs[row, col].grid(True, ls='--', color='gray', alpha=0.6)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{PATH4IMAGES}{model_name}_metrics_across_classes.jpg', format='jpg', dpi=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path Initialization and File Retrieval\n",
    "\n",
    "This cell initializes paths to the directory of model labels and images and retrieves a sorted list of specific CSV files located in the model labels directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files = sorted(glob.glob(f'{PATH_MODEL_LABELS}*.csv'))\n",
    "list_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Actual Labels\n",
    "\n",
    "This cell loads the actual labels of the test dataset from a .npy file located at a specified path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../2_training/data/datasets/test_dataset_labels.npy'\n",
    "labels = np.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame Creation for Actual Labels\n",
    "\n",
    "This cell creates a pandas DataFrame from the loaded labels, naming the column as actual_label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_labels = pd.DataFrame(labels, columns=['actual_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Replacement Dictionary Initialization\n",
    "\n",
    "This cell initializes a dictionary for replacing string class labels with corresponding numerical identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_dict = {\n",
    "    'None': 0,\n",
    "    'Pulse': 1,\n",
    "    'BBRFI': 2,\n",
    "    'NBRFI': 3,\n",
    "    'Pulse+BBRFI': 4,\n",
    "    'Pulse+NBRFI': 5,\n",
    "    'NBRFI+BBRFI': 6,\n",
    "    'Pulse+NBRFI+BBRFI': 7\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing String Labels with Numerical Identifiers\n",
    "\n",
    "This cell replaces the string labels in the actual_label column of the DataFrame with the numerical identifiers using the previously initialized dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_labels['actual_label'] = actual_labels['actual_label'].replace(replacement_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Visualization\n",
    "\n",
    "This cell evaluates each model using actual and predicted labels, calculates several metrics, and visualizes the confusion matrix for each class. It saves the metrics and the plots in specified paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = [\n",
    "    ['None', 'Pulse', 'BBRFI', 'NBRFI'],\n",
    "    ['Pulse+BBRFI', 'Pulse+NBRFI', 'BBRFI+NBRFI', 'Pulse+BBRFI+NBRFI']\n",
    "]\n",
    "    \n",
    "\n",
    "key_metrics_across_models = {}\n",
    "\n",
    "for file in list_files:\n",
    "    model_lables = pd.read_csv(file)[['models_label']]\n",
    "    model_name = os.path.basename(file).split('-')[0]\n",
    "    model_name = model_name[model_name.index('_p')+1:]\n",
    "    \n",
    "    table_for_comparison = actual_labels.merge(model_lables, left_index=True, right_index=True)\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for i in range(len(replacement_dict)):\n",
    "        y_true = table_for_comparison.actual_label\n",
    "        y_pred = table_for_comparison.models_label\n",
    "\n",
    "        tp, fp, fn, tn = get_metrics_for_class(y_true, y_pred, i)\n",
    "\n",
    "        data.append([\n",
    "            [tp, fp],\n",
    "            [fn, tn]\n",
    "        ])\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    F1_score_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "\n",
    "    precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    F1_score_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    key_metrics_across_models[model_name] = [\n",
    "        [accuracy, precision_micro, recall_micro, F1_score_micro],\n",
    "        [precision_macro, recall_macro, F1_score_macro]\n",
    "    ]\n",
    "    \n",
    "    \n",
    "            \n",
    "    fig, axs = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    fig.suptitle(model_name, fontsize=24, y=1.)\n",
    "    for i in range(2):\n",
    "        for j in range(4):\n",
    "            plot_confusion_matrix(data[i*4 + j], axs[i, j], title_list[i][j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{PATH4IMAGES}{model_name}_confusions_metrics.jpg', format='jpg', dpi=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Micro-average Metrics Visualization\n",
    "\n",
    "This cell visualizes and saves the bar plot of micro-average metrics like accuracy, recall, precision, and F1 score for each model using the plot_metrics function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['Accuracy', 'Recall', 'Precision', 'F1 score']\n",
    "\n",
    "plot_metrics([i[0] for i in key_metrics_across_models.values()], list(key_metrics_across_models.keys()), metrics, 'key_metrics_micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro-average Metrics Visualization\n",
    "\n",
    "This cell visualizes and saves the bar plot of macro-average metrics like recall, precision, and F1 score for each model using the plot_metrics function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['Recall', 'Precision', 'F1 score']\n",
    "\n",
    "plot_metrics([i[1] for i in key_metrics_across_models.values()], list(key_metrics_across_models.keys()), metrics, 'key_metrics_macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Calculation and Visualization for all Classes\n",
    "\n",
    "This cell calculates and plots precision, recall, and F1 score for all classes for each model, utilizing the plot_metrics_for_all_classes function. The plots are saved with specified filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in list_files:\n",
    "    model_lables = pd.read_csv(file)[['models_label']]\n",
    "    model_name = os.path.basename(file).split('-')[0]\n",
    "    model_name = model_name[model_name.index('prot'):]\n",
    "    \n",
    "    table_for_comparison = actual_labels.merge(model_lables, left_index=True, right_index=True)\n",
    "\n",
    "    y_true = table_for_comparison.actual_label\n",
    "    y_pred = table_for_comparison.models_label\n",
    "\n",
    "    plot_metrics_for_all_classes(y_true, y_pred, model_name, range(len(replacement_dict)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
