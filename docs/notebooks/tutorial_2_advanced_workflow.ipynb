{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5064a94",
   "metadata": {},
   "source": [
    "# Advanced Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588e0f6a",
   "metadata": {},
   "source": [
    "In the first two tutorials we saw how to define simple sequential workflows by\n",
    "means of the Pipeline object, which feds the outputs of the previous component\n",
    "as inputs of the following one.\n",
    "In this tutorial we show how to create more complex workflows, with\n",
    "non-sequential data flows. Here, components can be arranges as an directed\n",
    "acyclic graph (DAG). Under the DAG assumption, outputs of each block can be fed\n",
    "as input potentially to any other component, granting great flexibility to the\n",
    "experimenter.\n",
    "The trade-off for improved flexibility is a change in the way we define\n",
    "configuration files. From now on, it will only be possible to configure the\n",
    "parameters used by the training script, but not its structure through the\n",
    "Pipeline.\n",
    "\n",
    "itwinai provides a wrapper of jsonarparse's ArgumentParser which supports\n",
    "configuration files by default.\n",
    "\n",
    "To run as usual:\n",
    "> python my_script.py -d 20 --train-prop 0.7 --val-prop 0.2 --lr 1e-5\n",
    "\n",
    "To reuse the parameters saved in a configuration file and override some\n",
    "parameter (e.g., learning rate):\n",
    "> python my_script.py --config advanced_tutorial_conf.yaml --lr 2e-3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c30284ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from itwinai.parser import ArgumentParser\n",
    "from itwinai.components import Predictor, monitor_exec\n",
    "from typing import List, Optional, Tuple, Any\n",
    "from itwinai.components import (\n",
    "    DataGetter, DataSplitter, Trainer, Saver, monitor_exec\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47187fc",
   "metadata": {},
   "source": [
    "## Component interface implementation\n",
    "\n",
    "\n",
    "Here we show how to implement component interfaces in a simple way.\n",
    "\n",
    "New components are defined as classes inheriting from the BaseComponent class or\n",
    "an already defined component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d95a4909",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataGetter(DataGetter):\n",
    "    def __init__(self, data_size: int, name: Optional[str] = None) -> None:\n",
    "        super().__init__(name)\n",
    "        self.data_size = data_size\n",
    "        self.save_parameters(data_size=data_size)\n",
    "\n",
    "    @monitor_exec\n",
    "    def execute(self) -> List[int]:\n",
    "        \"\"\"Return a list dataset.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: dataset\n",
    "        \"\"\"\n",
    "        return list(range(self.data_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0938cb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDatasetSplitter(DataSplitter):\n",
    "    @monitor_exec\n",
    "    def execute(\n",
    "        self,\n",
    "        dataset: List[int]\n",
    "    ) -> Tuple[List[int], List[int], List[int]]:\n",
    "        \"\"\"Splits a list dataset into train, validation and test datasets.\n",
    "\n",
    "        Args:\n",
    "            dataset (List[int]): input list dataset.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[int], List[int], List[int]]: train, validation, and\n",
    "            test datasets respectively.\n",
    "        \"\"\"\n",
    "        train_n = int(len(dataset)*self.train_proportion)\n",
    "        valid_n = int(len(dataset)*self.validation_proportion)\n",
    "        train_set = dataset[:train_n]\n",
    "        vaild_set = dataset[train_n:train_n+valid_n]\n",
    "        test_set = dataset[train_n+valid_n:]\n",
    "        return train_set, vaild_set, test_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f11178",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainer(Trainer):\n",
    "    def __init__(self, lr: float = 1e-3, name: Optional[str] = None) -> None:\n",
    "        super().__init__(name)\n",
    "        self.save_parameters(name=name, lr=lr)\n",
    "\n",
    "    @monitor_exec\n",
    "    def execute(\n",
    "        self,\n",
    "        train_set: List[int],\n",
    "        vaild_set: List[int],\n",
    "        test_set: List[int]\n",
    "    ) -> Tuple[List[int], List[int], List[int], str]:\n",
    "        \"\"\"Dummy ML trainer mocking a ML training algorithm.\n",
    "\n",
    "        Args:\n",
    "            train_set (List[int]): training dataset.\n",
    "            vaild_set (List[int]): validation dataset.\n",
    "            test_set (List[int]): test dataset.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[int], List[int], List[int], str]: train, validation,\n",
    "            test datasets, and trained model.\n",
    "        \"\"\"\n",
    "        return train_set, vaild_set, test_set, \"my_trained_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c19b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySaver(Saver):\n",
    "    @monitor_exec\n",
    "    def execute(self, artifact: Any) -> Any:\n",
    "        \"\"\"Saves an artifact to disk.\n",
    "\n",
    "        Args:\n",
    "            artifact (Any): artifact to save (e.g., dataset, model).\n",
    "\n",
    "        Returns:\n",
    "            Any: input artifact.\n",
    "        \"\"\"\n",
    "        return artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "826fe8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnsemblePredictor(Predictor):\n",
    "    @monitor_exec\n",
    "    def execute(self, dataset, model_ensemble) -> Any:\n",
    "        \"\"\"\n",
    "        do some predictions with model on dataset...\n",
    "        \"\"\"\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66209fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser(description=\"itwinai advanced workflows tutorial\")\n",
    "parser.add_argument(\n",
    "    \"--data-size\", \"-d\", type=int, required=True,\n",
    "    help=\"Dataset cardinality.\")\n",
    "parser.add_argument(\n",
    "    \"--train-prop\", type=float, required=True,\n",
    "    help=\"Train split proportion.\")\n",
    "parser.add_argument(\n",
    "    \"--val-prop\", type=float, required=True,\n",
    "    help=\"Validation split proportion.\")\n",
    "parser.add_argument(\n",
    "    \"--lr\", type=float, help=\"Training learning rate.\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Save parsed arguments to configuration file.\n",
    "# Previous configurations are overwritten, which is not good,\n",
    "# but the versioning of configuration files is out of the scope\n",
    "# of this tutorial.\n",
    "parser.save(\n",
    "    args, \"advanced_tutorial_conf.yaml\", format='yaml', overwrite=True)\n",
    "\n",
    "# Define workflow components\n",
    "getter = MyDataGetter(data_size=args.data_size)\n",
    "splitter = MyDatasetSplitter(\n",
    "    train_proportion=args.train_prop,\n",
    "    validation_proportion=args.val_prop,\n",
    "    test_proportion=1-args.train_prop-args.val_prop\n",
    ")\n",
    "trainer1 = MyTrainer(lr=args.lr)\n",
    "trainer2 = MyTrainer(lr=args.lr)\n",
    "saver = MySaver()\n",
    "predictor = MyEnsemblePredictor(model=None)\n",
    "\n",
    "# Define ML workflow\n",
    "dataset = getter.execute()\n",
    "train_spl, val_spl, test_spl = splitter.execute(dataset)\n",
    "_, _, _, trained_model1 = trainer1.execute(train_spl, val_spl, test_spl)\n",
    "_, _, _, trained_model2 = trainer2.execute(train_spl, val_spl, test_spl)\n",
    "_ = saver.execute(trained_model1)\n",
    "predictions = predictor.execute(test_spl, [trained_model1, trained_model2])\n",
    "print()\n",
    "print(\"Predictions: \" + str(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0360c1f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
